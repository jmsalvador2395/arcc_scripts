loaded anaconda
loaded cuda
activated conda environment
changed directory to /home/jsalvador/projects/llmeval
------------------
*** EVALUATING allenai/OLMo-2-1124-7B-Instruct ***
WARNING 01-24 04:08:27 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-24 04:08:38,198	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-24 04:08:56 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-7B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-7B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-24 04:08:56 ray_gpu_executor.py:134] use_ray_spmd_worker: False
slurmstepd: error: *** JOB 413504 ON evc23 CANCELLED AT 2025-01-24T04:08:58 DUE TO NODE FAILURE, SEE SLURMCTLD LOG FOR DETAILS ***
INFO 01-24 04:09:35 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-24 04:09:35 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=68153)[0m INFO 01-24 04:09:35 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=68153)[0m INFO 01-24 04:09:35 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-24 04:09:35 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=68153)[0m INFO 01-24 04:09:35 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-24 04:09:35 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x143929ab0a50>, local_subscribe_port=56871, remote_subscribe_port=None)
INFO 01-24 04:09:36 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
[36m(RayWorkerWrapper pid=68153)[0m INFO 01-24 04:09:36 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
ERROR 01-24 04:09:36 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-24 04:09:36 worker_base.py:464] Traceback (most recent call last):
ERROR 01-24 04:09:36 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-24 04:09:36 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-24 04:09:36 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:09:36 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-24 04:09:36 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-24 04:09:36 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-24 04:09:36 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-24 04:09:36 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:09:36 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-24 04:09:36 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-24 04:09:36 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:09:36 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-24 04:09:36 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-24 04:09:36 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:09:36 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-24 04:09:36 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-24 04:09:36 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:09:36 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-24 04:09:36 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-24 04:09:36 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:09:36 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-24 04:09:36 worker_base.py:464]     raise ValueError(
ERROR 01-24 04:09:36 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-7B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-24 04:09:36,261	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=68153, ip=172.17.11.42, actor_id=42f528a45a7d336f25d2878701000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x14f79c663210>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=68153)[0m ERROR 01-24 04:09:36 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W124 04:09:39.593857669 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
WARNING 01-24 04:09:54 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-24 04:10:04,317	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-24 04:10:22 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-7B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-7B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-24 04:10:22 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-24 04:10:58 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-24 04:10:58 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=72573)[0m INFO 01-24 04:10:58 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=72573)[0m INFO 01-24 04:10:58 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-24 04:10:58 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-24 04:10:58 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x153789674750>, local_subscribe_port=49699, remote_subscribe_port=None)
INFO 01-24 04:10:58 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
[36m(RayWorkerWrapper pid=72573)[0m INFO 01-24 04:10:58 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=72573)[0m INFO 01-24 04:10:58 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
ERROR 01-24 04:10:58 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-24 04:10:58 worker_base.py:464] Traceback (most recent call last):
ERROR 01-24 04:10:58 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-24 04:10:58 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-24 04:10:58 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:10:58 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-24 04:10:58 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-24 04:10:58 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-24 04:10:58 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-24 04:10:58 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:10:58 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-24 04:10:58 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-24 04:10:58 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:10:58 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-24 04:10:58 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-24 04:10:58 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:10:58 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-24 04:10:58 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-24 04:10:58 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:10:58 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-24 04:10:58 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-24 04:10:58 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:10:58 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-24 04:10:58 worker_base.py:464]     raise ValueError(
ERROR 01-24 04:10:58 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-7B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-24 04:10:59,057	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=72573, ip=172.17.11.42, actor_id=0e3f67bcab9d7c587084481901000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x14f39814fe10>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=72573)[0m ERROR 01-24 04:10:58 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W124 04:11:01.087247066 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
WARNING 01-24 04:11:15 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-24 04:11:25,589	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-24 04:11:43 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-7B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-7B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-24 04:11:43 ray_gpu_executor.py:134] use_ray_spmd_worker: False
slurmstepd: error: *** JOB 413504 STEPD TERMINATED ON evc23 AT 2025-01-24T04:12:01 DUE TO JOB NOT ENDING WITH SIGNALS ***
INFO 01-24 04:12:17 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-24 04:12:17 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=76986)[0m INFO 01-24 04:12:17 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=76986)[0m INFO 01-24 04:12:17 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-24 04:12:18 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-24 04:12:18 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x14d2114e8750>, local_subscribe_port=48735, remote_subscribe_port=None)
INFO 01-24 04:12:18 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
[36m(RayWorkerWrapper pid=76986)[0m INFO 01-24 04:12:18 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=76986)[0m INFO 01-24 04:12:18 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
ERROR 01-24 04:12:18 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-24 04:12:18 worker_base.py:464] Traceback (most recent call last):
ERROR 01-24 04:12:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-24 04:12:18 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-24 04:12:18 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:12:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-24 04:12:18 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-24 04:12:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-24 04:12:18 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-24 04:12:18 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:12:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-24 04:12:18 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-24 04:12:18 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:12:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-24 04:12:18 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-24 04:12:18 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:12:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-24 04:12:18 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-24 04:12:18 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:12:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-24 04:12:18 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-24 04:12:18 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:12:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-24 04:12:18 worker_base.py:464]     raise ValueError(
ERROR 01-24 04:12:18 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-7B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-24 04:12:18,685	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=76986, ip=172.17.11.42, actor_id=a8173413e9d603662cfffbb201000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x14e61d0e3390>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=76986)[0m ERROR 01-24 04:12:18 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W124 04:12:21.526780934 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
WARNING 01-24 04:12:35 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-24 04:12:45,325	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-24 04:13:03 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-7B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-7B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-24 04:13:03 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-24 04:13:38 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-24 04:13:38 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=81449)[0m INFO 01-24 04:13:38 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=81449)[0m INFO 01-24 04:13:38 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-24 04:13:39 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-24 04:13:39 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x150635f744d0>, local_subscribe_port=38629, remote_subscribe_port=None)
INFO 01-24 04:13:39 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
[36m(RayWorkerWrapper pid=81449)[0m INFO 01-24 04:13:39 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=81449)[0m INFO 01-24 04:13:39 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
ERROR 01-24 04:13:39 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-24 04:13:39 worker_base.py:464] Traceback (most recent call last):
ERROR 01-24 04:13:39 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-24 04:13:39 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-24 04:13:39 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:13:39 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-24 04:13:39 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-24 04:13:39 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-24 04:13:39 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-24 04:13:39 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:13:39 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-24 04:13:39 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-24 04:13:39 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:13:39 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-24 04:13:39 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-24 04:13:39 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:13:39 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-24 04:13:39 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-24 04:13:39 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:13:39 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-24 04:13:39 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-24 04:13:39 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:13:39 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-24 04:13:39 worker_base.py:464]     raise ValueError(
ERROR 01-24 04:13:39 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-7B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-24 04:13:39,310	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=81449, ip=172.17.11.42, actor_id=b9ec769ff88083a7f592013601000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x1499ef71ba50>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=81449)[0m ERROR 01-24 04:13:39 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W124 04:13:42.359816624 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
*** EVALUATING allenai/OLMo-2-1124-13B-Instruct ***
WARNING 01-24 04:13:56 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-24 04:14:06,218	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-24 04:14:24 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-13B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-13B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-13B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-24 04:14:24 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-24 04:14:59 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-24 04:14:59 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=85860)[0m INFO 01-24 04:14:59 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=85860)[0m INFO 01-24 04:14:59 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-24 04:15:00 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-24 04:15:00 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x149ec01f0f50>, local_subscribe_port=46161, remote_subscribe_port=None)
INFO 01-24 04:15:00 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
[36m(RayWorkerWrapper pid=85860)[0m INFO 01-24 04:15:00 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=85860)[0m INFO 01-24 04:15:00 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
ERROR 01-24 04:15:00 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-24 04:15:00 worker_base.py:464] Traceback (most recent call last):
ERROR 01-24 04:15:00 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-24 04:15:00 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-24 04:15:00 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:15:00 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-24 04:15:00 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-24 04:15:00 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-24 04:15:00 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-24 04:15:00 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:15:00 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-24 04:15:00 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-24 04:15:00 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:15:00 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-24 04:15:00 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-24 04:15:00 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:15:00 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-24 04:15:00 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-24 04:15:00 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:15:00 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-24 04:15:00 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-24 04:15:00 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:15:00 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-24 04:15:00 worker_base.py:464]     raise ValueError(
ERROR 01-24 04:15:00 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-13B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-24 04:15:00,270	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=85860, ip=172.17.11.42, actor_id=8ce715980538e3317eac207801000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x14b8b7f6bc10>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=85860)[0m ERROR 01-24 04:15:00 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W124 04:15:03.436793359 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
WARNING 01-24 04:15:16 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-24 04:15:26,142	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-24 04:15:44 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-13B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-13B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-13B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-24 04:15:44 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-24 04:16:18 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-24 04:16:18 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=90305)[0m INFO 01-24 04:16:18 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=90305)[0m INFO 01-24 04:16:18 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-24 04:16:19 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-24 04:16:19 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x15524a83d750>, local_subscribe_port=56699, remote_subscribe_port=None)
INFO 01-24 04:16:19 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
ERROR 01-24 04:16:19 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-24 04:16:19 worker_base.py:464] Traceback (most recent call last):
ERROR 01-24 04:16:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-24 04:16:19 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-24 04:16:19 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:16:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-24 04:16:19 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-24 04:16:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-24 04:16:19 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-24 04:16:19 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:16:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-24 04:16:19 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-24 04:16:19 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:16:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-24 04:16:19 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-24 04:16:19 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:16:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-24 04:16:19 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-24 04:16:19 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:16:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-24 04:16:19 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-24 04:16:19 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:16:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-24 04:16:19 worker_base.py:464]     raise ValueError(
ERROR 01-24 04:16:19 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-13B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-24 04:16:19,360	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=90305, ip=172.17.11.42, actor_id=c522dbda2af3615d62078b3e01000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x146dc2a6ba10>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=90305)[0m INFO 01-24 04:16:19 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=90305)[0m INFO 01-24 04:16:19 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=90305)[0m ERROR 01-24 04:16:19 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W124 04:16:22.674740259 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
WARNING 01-24 04:16:36 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-24 04:16:46,181	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-24 04:17:04 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-13B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-13B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-13B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-24 04:17:04 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-24 04:17:40 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-24 04:17:40 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=94732)[0m INFO 01-24 04:17:40 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=94732)[0m INFO 01-24 04:17:40 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-24 04:17:41 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-24 04:17:41 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x14dcdd94e350>, local_subscribe_port=34781, remote_subscribe_port=None)
INFO 01-24 04:17:41 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
[36m(RayWorkerWrapper pid=94732)[0m INFO 01-24 04:17:41 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
ERROR 01-24 04:17:41 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-24 04:17:41 worker_base.py:464] Traceback (most recent call last):
ERROR 01-24 04:17:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-24 04:17:41 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-24 04:17:41 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:17:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-24 04:17:41 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-24 04:17:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-24 04:17:41 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-24 04:17:41 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:17:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-24 04:17:41 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-24 04:17:41 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:17:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-24 04:17:41 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-24 04:17:41 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:17:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-24 04:17:41 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-24 04:17:41 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:17:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-24 04:17:41 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-24 04:17:41 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:17:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-24 04:17:41 worker_base.py:464]     raise ValueError(
ERROR 01-24 04:17:41 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-13B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-24 04:17:41,383	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=94732, ip=172.17.11.42, actor_id=ee7eb9fe943bbc4b1e17b24d01000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x14bd0ec2fcd0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=94732)[0m INFO 01-24 04:17:41 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=94732)[0m ERROR 01-24 04:17:41 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W124 04:17:44.357672425 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
WARNING 01-24 04:17:57 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-24 04:18:07,912	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-24 04:18:25 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-13B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-13B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-13B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-24 04:18:26 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-24 04:19:02 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-24 04:19:02 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=99156)[0m INFO 01-24 04:19:02 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=99156)[0m INFO 01-24 04:19:02 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-24 04:19:02 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=99156)[0m INFO 01-24 04:19:02 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-24 04:19:02 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x14539a15c210>, local_subscribe_port=41713, remote_subscribe_port=None)
INFO 01-24 04:19:02 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
ERROR 01-24 04:19:03 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-24 04:19:03 worker_base.py:464] Traceback (most recent call last):
ERROR 01-24 04:19:03 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-24 04:19:03 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-24 04:19:03 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:19:03 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-24 04:19:03 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-24 04:19:03 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-24 04:19:03 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-24 04:19:03 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:19:03 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-24 04:19:03 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-24 04:19:03 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:19:03 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-24 04:19:03 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-24 04:19:03 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:19:03 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-24 04:19:03 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-24 04:19:03 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:19:03 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-24 04:19:03 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-24 04:19:03 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 04:19:03 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-24 04:19:03 worker_base.py:464]     raise ValueError(
ERROR 01-24 04:19:03 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-13B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-24 04:19:03,241	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=99156, ip=172.17.11.42, actor_id=50c4b435dff1b46b8070275b01000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x14b60bd492d0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=99156)[0m INFO 01-24 04:19:02 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=99156)[0m ERROR 01-24 04:19:03 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W124 04:19:06.442971804 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
*** EVALUATING google/gemma-2-2b-it ***
WARNING 01-24 04:19:19 utils.py:747] Gemma 2 uses sliding window attention for every odd layer, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).
WARNING 01-24 04:19:19 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-24 04:19:30,142	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-24 04:19:47 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='google/gemma-2-2b-it', speculative_config=None, tokenizer='google/gemma-2-2b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2-2b-it, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-24 04:19:48 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-24 04:20:26 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-24 04:20:26 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=103585)[0m INFO 01-24 04:20:26 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=103585)[0m INFO 01-24 04:20:26 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-24 04:20:26 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-24 04:20:26 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x14693da00390>, local_subscribe_port=57147, remote_subscribe_port=None)
INFO 01-24 04:20:26 model_runner.py:1014] Starting to load model google/gemma-2-2b-it...
[36m(RayWorkerWrapper pid=103585)[0m INFO 01-24 04:20:26 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=103585)[0m INFO 01-24 04:20:26 model_runner.py:1014] Starting to load model google/gemma-2-2b-it...
INFO 01-24 04:20:26 weight_utils.py:242] Using model weights format ['*.safetensors']
[36m(RayWorkerWrapper pid=103585)[0m INFO 01-24 04:20:26 weight_utils.py:242] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]

INFO 01-24 04:22:28 model_runner.py:1025] Loading model weights took 2.4389 GB
[36m(RayWorkerWrapper pid=103585)[0m INFO 01-24 04:22:28 model_runner.py:1025] Loading model weights took 2.4389 GB
INFO 01-24 04:22:38 distributed_gpu_executor.py:57] # GPU blocks: 82342, # CPU blocks: 5041
[36m(pid=103594)[0m [2025-01-24 04:31:00,106 E 103594 104763] gcs_rpc_client.h:664: Failed to connect to GCS within 60 seconds. GCS may have been killed. It's either GCS is terminated by `ray stop` or is killed unexpectedly. If it is killed unexpectedly, see the log file gcs_server.out. https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure. The program will terminate.
[2025-01-24 04:46:49,019 E 103155 107235] gcs_rpc_client.h:664: Failed to connect to GCS within 60 seconds. GCS may have been killed. It's either GCS is terminated by `ray stop` or is killed unexpectedly. If it is killed unexpectedly, see the log file gcs_server.out. https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure. The program will terminate.
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
WARNING 01-24 04:47:10 utils.py:747] Gemma 2 uses sliding window attention for every odd layer, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).
WARNING 01-24 04:47:10 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-24 04:47:21,089	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-24 04:47:38 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='google/gemma-2-2b-it', speculative_config=None, tokenizer='google/gemma-2-2b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2-2b-it, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-24 04:47:39 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-24 04:48:14 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-24 04:48:14 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=108282)[0m INFO 01-24 04:48:14 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=108282)[0m INFO 01-24 04:48:14 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-24 04:48:14 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-24 04:48:14 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x14fe064a4050>, local_subscribe_port=42081, remote_subscribe_port=None)
INFO 01-24 04:48:14 model_runner.py:1014] Starting to load model google/gemma-2-2b-it...
[36m(RayWorkerWrapper pid=108282)[0m INFO 01-24 04:48:14 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=108282)[0m INFO 01-24 04:48:14 model_runner.py:1014] Starting to load model google/gemma-2-2b-it...
INFO 01-24 04:48:14 weight_utils.py:242] Using model weights format ['*.safetensors']
[36m(RayWorkerWrapper pid=108282)[0m INFO 01-24 04:48:14 weight_utils.py:242] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.20it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  3.15s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.81s/it]

INFO 01-24 04:48:21 model_runner.py:1025] Loading model weights took 2.4389 GB
[36m(RayWorkerWrapper pid=108282)[0m INFO 01-24 04:48:20 model_runner.py:1025] Loading model weights took 2.4389 GB
INFO 01-24 04:48:29 distributed_gpu_executor.py:57] # GPU blocks: 82342, # CPU blocks: 5041
slurmstepd: error: Detected 1 oom_kill event in StepId=413504.batch. Some of the step tasks have been OOM Killed.
slurmstepd: error: *** JOB 413504 STEPD TERMINATED ON evc48 AT 2025-01-24T05:52:37 DUE TO JOB NOT ENDING WITH SIGNALS ***
slurmstepd: error: Container 1354438 in cgroup plugin has 69 processes, giving up after 127 sec
