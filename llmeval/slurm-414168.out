loaded anaconda
loaded cuda
activated conda environment
changed directory to /home/jsalvador/projects/llmeval
[[33mINFO[0m] running infill_solve using meta-llama/Llama-3.3-70B-Instruct
[[33mINFO[0m] fetching subsample IDs
counting prompt variations
[[33mINFO[0m] unique template_name query took 1777.20 seconds
[[33mINFO[0m] unique template_id query took 2098.28 seconds
[[33mINFO[0m] unique sys_id query took 28.82 seconds
fitb_l0, tids: 1, sids: 6
[[33mINFO[0m] counting samples from each query
[[33mINFO[0m] counting samples took 0.17 seconds
[[33mINFO[0m] unique template_id query took 28.09 seconds
[[33mINFO[0m] unique sys_id query took 27.58 seconds
fitb_l1, tids: 4, sids: 6
[[33mINFO[0m] counting samples from each query
[[33mINFO[0m] counting samples took 0.21 seconds
[[33mINFO[0m] unique template_id query took 28.21 seconds
[[33mINFO[0m] unique sys_id query took 27.49 seconds
fitb_l2, tids: 5, sids: 6
[[33mINFO[0m] counting samples from each query
[[33mINFO[0m] counting samples took 0.22 seconds
[[33mINFO[0m] unique template_id query took 27.43 seconds
[[33mINFO[0m] unique sys_id query took 27.44 seconds
fitb_l3, tids: 5, sids: 6
[[33mINFO[0m] counting samples from each query
[[33mINFO[0m] counting samples took 0.23 seconds
[[33mINFO[0m] unique template_id query took 28.28 seconds
[[33mINFO[0m] unique sys_id query took 27.23 seconds
fitb_l4, tids: 6, sids: 6
[[33mINFO[0m] counting samples from each query
[[33mINFO[0m] counting samples took 0.26 seconds
[[33mINFO[0m] batch_size: 1000, limit: 1000, N: 126000, N_batch: 126
creating generator function
[[33mINFO[0m] loading model: meta-llama/Llama-3.3-70B-Instruct
WARNING 01-28 00:56:05 config.py:319] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.
WARNING 01-28 00:56:05 arg_utils.py:930] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 01-28 00:56:05 config.py:1010] Chunked prefill is enabled with max_num_batched_tokens=512.
INFO 01-28 00:56:05 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=50000, download_dir='/lustre/fs1/home/jsalvador/projects/llmeval/data/llm_cache', load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1738043765, served_model_name=meta-llama/Llama-3.3-70B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-28 00:56:23 model_runner.py:1014] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
INFO 01-28 00:56:28 loader.py:1014] Loading weights with BitsAndBytes quantization.  May take a while ...
INFO 01-28 00:56:28 weight_utils.py:242] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   3% Completed | 1/30 [00:07<03:26,  7.11s/it]
Loading safetensors checkpoint shards:   7% Completed | 2/30 [00:12<02:46,  5.95s/it]
Loading safetensors checkpoint shards:  10% Completed | 3/30 [00:17<02:29,  5.53s/it]
Loading safetensors checkpoint shards:  13% Completed | 4/30 [00:21<02:11,  5.07s/it]
Loading safetensors checkpoint shards:  17% Completed | 5/30 [00:25<01:54,  4.59s/it]
Loading safetensors checkpoint shards:  20% Completed | 6/30 [00:30<01:52,  4.67s/it]
Loading safetensors checkpoint shards:  23% Completed | 7/30 [00:35<01:50,  4.80s/it]
Loading safetensors checkpoint shards:  27% Completed | 8/30 [00:40<01:49,  4.96s/it]
Loading safetensors checkpoint shards:  30% Completed | 9/30 [00:46<01:47,  5.13s/it]
Loading safetensors checkpoint shards:  33% Completed | 10/30 [00:51<01:43,  5.18s/it]
Loading safetensors checkpoint shards:  37% Completed | 11/30 [00:54<01:23,  4.42s/it]
Loading safetensors checkpoint shards:  40% Completed | 12/30 [00:59<01:25,  4.73s/it]
Loading safetensors checkpoint shards:  43% Completed | 13/30 [01:04<01:20,  4.73s/it]
Loading safetensors checkpoint shards:  47% Completed | 14/30 [01:09<01:16,  4.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 15/30 [01:14<01:13,  4.91s/it]
Loading safetensors checkpoint shards:  53% Completed | 16/30 [01:19<01:07,  4.85s/it]
Loading safetensors checkpoint shards:  57% Completed | 17/30 [01:24<01:03,  4.91s/it]
Loading safetensors checkpoint shards:  60% Completed | 18/30 [01:28<00:57,  4.83s/it]
Loading safetensors checkpoint shards:  63% Completed | 19/30 [01:34<00:56,  5.09s/it]
Loading safetensors checkpoint shards:  67% Completed | 20/30 [01:38<00:48,  4.81s/it]
Loading safetensors checkpoint shards:  70% Completed | 21/30 [01:44<00:45,  5.02s/it]
Loading safetensors checkpoint shards:  73% Completed | 22/30 [01:48<00:38,  4.78s/it]
Loading safetensors checkpoint shards:  77% Completed | 23/30 [01:52<00:32,  4.69s/it]
Loading safetensors checkpoint shards:  80% Completed | 24/30 [01:57<00:27,  4.62s/it]
Loading safetensors checkpoint shards:  83% Completed | 25/30 [02:02<00:24,  4.91s/it]
Loading safetensors checkpoint shards:  87% Completed | 26/30 [02:08<00:20,  5.16s/it]
Loading safetensors checkpoint shards:  90% Completed | 27/30 [02:13<00:14,  4.96s/it]
Loading safetensors checkpoint shards:  93% Completed | 28/30 [02:17<00:09,  4.79s/it]
Loading safetensors checkpoint shards:  97% Completed | 29/30 [02:22<00:04,  4.87s/it]
Loading safetensors checkpoint shards: 100% Completed | 30/30 [02:27<00:00,  4.84s/it]
Loading safetensors checkpoint shards: 100% Completed | 30/30 [02:27<00:00,  4.91s/it]

INFO 01-28 00:58:56 model_runner.py:1025] Loading model weights took 36.8384 GB
INFO 01-28 00:59:04 gpu_executor.py:122] # GPU blocks: 6726, # CPU blocks: 819
INFO 01-28 00:59:06 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-28 00:59:06 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 01-28 00:59:47 model_runner.py:1456] Graph capturing finished in 41 secs.
[[33mINFO[0m] begin collecting responses
generating responses:   0%|          | 0/378 [00:00<?, ?it/s]generating responses:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 371/378 [36:33<00:41,  5.91s/it]WARNING 01-28 02:04:13 scheduler.py:1439] Sequence group 1913 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1
generating responses:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 372/378 [1:14:45<01:28, 14.68s/it]generating responses:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 373/378 [1:49:53<02:10, 26.09s/it]generating responses:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 374/378 [2:21:48<02:42, 40.68s/it]generating responses:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 375/378 [2:55:56<03:07, 62.59s/it]generating responses:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 376/378 [3:24:07<02:55, 87.60s/it]generating responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 377/378 [3:56:26<02:07, 127.34s/it]generating responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 378/378 [4:35:48<00:00, 193.82s/it]generating responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 378/378 [4:35:48<00:00, 43.78s/it] 
