loaded anaconda
loaded cuda
activated conda environment
changed directory to /home/jsalvador/projects/llmeval
------------------
*** EVALUATING allenai/OLMo-2-1124-7B-Instruct ***
WARNING 01-23 10:48:13 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-23 10:48:23,238	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-23 10:48:41 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-7B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-7B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-23 10:48:42 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-23 10:49:18 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-23 10:49:18 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=3173322)[0m INFO 01-23 10:49:18 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=3173322)[0m INFO 01-23 10:49:18 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-23 10:49:19 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 10:49:19 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x14c677e2c790>, local_subscribe_port=53087, remote_subscribe_port=None)
INFO 01-23 10:49:19 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
[36m(RayWorkerWrapper pid=3173322)[0m INFO 01-23 10:49:19 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=3173322)[0m INFO 01-23 10:49:20 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
ERROR 01-23 10:49:20 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-23 10:49:20 worker_base.py:464] Traceback (most recent call last):
ERROR 01-23 10:49:20 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-23 10:49:20 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-23 10:49:20 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:49:20 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-23 10:49:20 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-23 10:49:20 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-23 10:49:20 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-23 10:49:20 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:49:20 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-23 10:49:20 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-23 10:49:20 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:49:20 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-23 10:49:20 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-23 10:49:20 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:49:20 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-23 10:49:20 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-23 10:49:20 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:49:20 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-23 10:49:20 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-23 10:49:20 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:49:20 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-23 10:49:20 worker_base.py:464]     raise ValueError(
ERROR 01-23 10:49:20 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-7B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-23 10:49:20,251	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=3173322, ip=172.17.11.31, actor_id=0ce513b9bc9708808f1e69e501000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x14fb0e9b01d0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=3173322)[0m ERROR 01-23 10:49:20 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W123 10:49:23.282835407 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
WARNING 01-23 10:49:37 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-23 10:49:47,995	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-23 10:50:06 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-7B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-7B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-23 10:50:06 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-23 10:50:43 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-23 10:50:43 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=3177780)[0m INFO 01-23 10:50:43 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=3177780)[0m INFO 01-23 10:50:43 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-23 10:50:44 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 10:50:44 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x14c7a8128550>, local_subscribe_port=60921, remote_subscribe_port=None)
INFO 01-23 10:50:44 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
[36m(RayWorkerWrapper pid=3177780)[0m INFO 01-23 10:50:44 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=3177780)[0m INFO 01-23 10:50:44 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
ERROR 01-23 10:50:44 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-23 10:50:44 worker_base.py:464] Traceback (most recent call last):
ERROR 01-23 10:50:44 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-23 10:50:44 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-23 10:50:44 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:50:44 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-23 10:50:44 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-23 10:50:44 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-23 10:50:44 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-23 10:50:44 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:50:44 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-23 10:50:44 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-23 10:50:44 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:50:44 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-23 10:50:44 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-23 10:50:44 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:50:44 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-23 10:50:44 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-23 10:50:44 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:50:44 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-23 10:50:44 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-23 10:50:44 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:50:44 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-23 10:50:44 worker_base.py:464]     raise ValueError(
ERROR 01-23 10:50:44 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-7B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-23 10:50:44,401	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=3177780, ip=172.17.11.31, actor_id=7c5c43d33e1181ff206f325701000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x15102b3aa610>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=3177780)[0m ERROR 01-23 10:50:44 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W123 10:50:47.498759933 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
WARNING 01-23 10:51:01 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-23 10:51:11,362	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-23 10:51:29 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-7B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-7B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-23 10:51:29 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-23 10:52:05 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-23 10:52:05 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=3182222)[0m INFO 01-23 10:52:05 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=3182222)[0m INFO 01-23 10:52:05 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-23 10:52:05 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=3182222)[0m INFO 01-23 10:52:05 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 10:52:05 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x1493e5698550>, local_subscribe_port=45763, remote_subscribe_port=None)
INFO 01-23 10:52:05 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
[36m(RayWorkerWrapper pid=3182222)[0m INFO 01-23 10:52:05 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
ERROR 01-23 10:52:05 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-23 10:52:05 worker_base.py:464] Traceback (most recent call last):
ERROR 01-23 10:52:05 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-23 10:52:05 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-23 10:52:05 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:52:05 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-23 10:52:05 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-23 10:52:05 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-23 10:52:05 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-23 10:52:05 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:52:05 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-23 10:52:05 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-23 10:52:05 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:52:05 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-23 10:52:05 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-23 10:52:05 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:52:05 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-23 10:52:05 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-23 10:52:05 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:52:05 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-23 10:52:05 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-23 10:52:05 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:52:05 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-23 10:52:05 worker_base.py:464]     raise ValueError(
ERROR 01-23 10:52:05 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-7B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-23 10:52:05,945	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=3182222, ip=172.17.11.31, actor_id=eae8d278fc41e691a8b7933d01000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x14702d58b110>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=3182222)[0m ERROR 01-23 10:52:05 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W123 10:52:08.055603182 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
WARNING 01-23 10:52:22 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-23 10:52:32,840	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-23 10:52:50 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-7B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-7B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-23 10:52:51 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-23 10:53:26 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-23 10:53:26 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=3186650)[0m INFO 01-23 10:53:26 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=3186650)[0m INFO 01-23 10:53:26 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-23 10:53:26 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 10:53:26 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x14a6064b0410>, local_subscribe_port=51733, remote_subscribe_port=None)
INFO 01-23 10:53:26 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
[36m(RayWorkerWrapper pid=3186650)[0m INFO 01-23 10:53:26 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=3186650)[0m INFO 01-23 10:53:26 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
ERROR 01-23 10:53:27 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-23 10:53:27 worker_base.py:464] Traceback (most recent call last):
ERROR 01-23 10:53:27 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-23 10:53:27 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-23 10:53:27 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:53:27 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-23 10:53:27 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-23 10:53:27 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-23 10:53:27 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-23 10:53:27 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:53:27 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-23 10:53:27 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-23 10:53:27 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:53:27 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-23 10:53:27 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-23 10:53:27 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:53:27 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-23 10:53:27 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-23 10:53:27 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:53:27 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-23 10:53:27 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-23 10:53:27 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:53:27 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-23 10:53:27 worker_base.py:464]     raise ValueError(
ERROR 01-23 10:53:27 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-7B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-23 10:53:27,240	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=3186650, ip=172.17.11.31, actor_id=c1bee2a528cfd352914c881601000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x14d030f8dc10>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=3186650)[0m ERROR 01-23 10:53:27 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W123 10:53:30.899290557 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
*** EVALUATING allenai/OLMo-2-1124-13B-Instruct ***
WARNING 01-23 10:53:44 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-23 10:53:54,800	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-23 10:54:12 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-13B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-13B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-13B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-23 10:54:13 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-23 10:54:47 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-23 10:54:47 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=3191117)[0m INFO 01-23 10:54:47 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=3191117)[0m INFO 01-23 10:54:47 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-23 10:54:47 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 10:54:47 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x1473924ab250>, local_subscribe_port=40075, remote_subscribe_port=None)
[36m(RayWorkerWrapper pid=3191117)[0m INFO 01-23 10:54:47 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 10:54:47 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
[36m(RayWorkerWrapper pid=3191117)[0m INFO 01-23 10:54:47 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
ERROR 01-23 10:54:48 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-23 10:54:48 worker_base.py:464] Traceback (most recent call last):
ERROR 01-23 10:54:48 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-23 10:54:48 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-23 10:54:48 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:54:48 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-23 10:54:48 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-23 10:54:48 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-23 10:54:48 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-23 10:54:48 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:54:48 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-23 10:54:48 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-23 10:54:48 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:54:48 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-23 10:54:48 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-23 10:54:48 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:54:48 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-23 10:54:48 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-23 10:54:48 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:54:48 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-23 10:54:48 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-23 10:54:48 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:54:48 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-23 10:54:48 worker_base.py:464]     raise ValueError(
ERROR 01-23 10:54:48 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-13B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-23 10:54:48,184	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=3191117, ip=172.17.11.31, actor_id=a8561fc545d76149a7ed502e01000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x14df77d34750>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=3191117)[0m ERROR 01-23 10:54:48 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W123 10:54:51.163566846 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
WARNING 01-23 10:55:04 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-23 10:55:14,949	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-23 10:55:32 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-13B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-13B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-13B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-23 10:55:33 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-23 10:56:08 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=3195531)[0m INFO 01-23 10:56:08 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=3195531)[0m INFO 01-23 10:56:08 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-23 10:56:08 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-23 10:56:09 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 10:56:09 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x1511d7fc4a10>, local_subscribe_port=46381, remote_subscribe_port=None)
INFO 01-23 10:56:09 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
[36m(RayWorkerWrapper pid=3195531)[0m INFO 01-23 10:56:09 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=3195531)[0m INFO 01-23 10:56:09 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
ERROR 01-23 10:56:09 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-23 10:56:09 worker_base.py:464] Traceback (most recent call last):
ERROR 01-23 10:56:09 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-23 10:56:09 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-23 10:56:09 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:56:09 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-23 10:56:09 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-23 10:56:09 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-23 10:56:09 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-23 10:56:09 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:56:09 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-23 10:56:09 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-23 10:56:09 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:56:09 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-23 10:56:09 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-23 10:56:09 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:56:09 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-23 10:56:09 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-23 10:56:09 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:56:09 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-23 10:56:09 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-23 10:56:09 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:56:09 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-23 10:56:09 worker_base.py:464]     raise ValueError(
ERROR 01-23 10:56:09 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-13B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-23 10:56:09,536	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=3195531, ip=172.17.11.31, actor_id=f333f4c2b714a7c67ed73a2e01000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x14e6261a8dd0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=3195531)[0m ERROR 01-23 10:56:09 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W123 10:56:12.582674977 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
WARNING 01-23 10:56:26 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-23 10:56:36,481	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-23 10:56:54 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-13B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-13B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-13B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-23 10:56:54 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-23 10:57:30 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-23 10:57:30 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=3199952)[0m INFO 01-23 10:57:30 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=3199952)[0m INFO 01-23 10:57:30 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-23 10:57:30 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 10:57:30 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x14be4fa7c490>, local_subscribe_port=54801, remote_subscribe_port=None)
INFO 01-23 10:57:30 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
[36m(RayWorkerWrapper pid=3199952)[0m INFO 01-23 10:57:30 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=3199952)[0m INFO 01-23 10:57:30 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
ERROR 01-23 10:57:30 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-23 10:57:30 worker_base.py:464] Traceback (most recent call last):
ERROR 01-23 10:57:30 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-23 10:57:30 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-23 10:57:30 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:57:30 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-23 10:57:30 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-23 10:57:30 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-23 10:57:30 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-23 10:57:30 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:57:30 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-23 10:57:30 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-23 10:57:30 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:57:30 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-23 10:57:30 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-23 10:57:30 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:57:30 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-23 10:57:30 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-23 10:57:30 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:57:30 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-23 10:57:30 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-23 10:57:30 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:57:30 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-23 10:57:30 worker_base.py:464]     raise ValueError(
ERROR 01-23 10:57:30 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-13B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-23 10:57:30,869	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=3199952, ip=172.17.11.31, actor_id=63da627e02872281aee90d7101000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x1541f78ead10>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=3199952)[0m ERROR 01-23 10:57:30 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W123 10:57:33.993722588 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
WARNING 01-23 10:57:47 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-23 10:57:57,832	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-23 10:58:15 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-13B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-13B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-13B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-23 10:58:16 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-23 10:58:52 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-23 10:58:52 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=3204370)[0m INFO 01-23 10:58:52 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=3204370)[0m INFO 01-23 10:58:52 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-23 10:58:53 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 10:58:53 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x14d7dd664d10>, local_subscribe_port=41613, remote_subscribe_port=None)
INFO 01-23 10:58:53 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
[36m(RayWorkerWrapper pid=3204370)[0m INFO 01-23 10:58:53 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=3204370)[0m INFO 01-23 10:58:53 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
ERROR 01-23 10:58:53 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-23 10:58:53 worker_base.py:464] Traceback (most recent call last):
ERROR 01-23 10:58:53 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-23 10:58:53 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-23 10:58:53 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:58:53 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-23 10:58:53 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-23 10:58:53 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-23 10:58:53 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-23 10:58:53 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:58:53 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-23 10:58:53 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-23 10:58:53 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:58:53 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-23 10:58:53 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-23 10:58:53 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:58:53 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-23 10:58:53 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-23 10:58:53 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:58:53 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-23 10:58:53 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-23 10:58:53 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 10:58:53 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-23 10:58:53 worker_base.py:464]     raise ValueError(
ERROR 01-23 10:58:53 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-13B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-23 10:58:53,956	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=3204370, ip=172.17.11.31, actor_id=2796dc9b1453285faaf49a4a01000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x14df7ed91b50>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=3204370)[0m ERROR 01-23 10:58:53 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W123 10:58:56.861987735 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
*** EVALUATING google/gemma-2-2b-it ***
WARNING 01-23 10:59:10 utils.py:747] Gemma 2 uses sliding window attention for every odd layer, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).
WARNING 01-23 10:59:10 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-23 10:59:20,680	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-23 10:59:38 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='google/gemma-2-2b-it', speculative_config=None, tokenizer='google/gemma-2-2b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2-2b-it, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-23 10:59:40 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-23 11:00:17 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-23 11:00:17 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=3208792)[0m INFO 01-23 11:00:17 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=3208792)[0m INFO 01-23 11:00:17 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-23 11:00:17 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=3208792)[0m INFO 01-23 11:00:17 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 11:00:17 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x1514f5043c10>, local_subscribe_port=56543, remote_subscribe_port=None)
INFO 01-23 11:00:17 model_runner.py:1014] Starting to load model google/gemma-2-2b-it...
[36m(RayWorkerWrapper pid=3208792)[0m INFO 01-23 11:00:17 model_runner.py:1014] Starting to load model google/gemma-2-2b-it...
INFO 01-23 11:00:18 weight_utils.py:242] Using model weights format ['*.safetensors']
[36m(RayWorkerWrapper pid=3208792)[0m INFO 01-23 11:00:18 weight_utils.py:242] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  8.23it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.16it/s]

[36m(RayWorkerWrapper pid=3208792)[0m INFO 01-23 11:02:21 model_runner.py:1025] Loading model weights took 2.4389 GB
INFO 01-23 11:02:21 model_runner.py:1025] Loading model weights took 2.4389 GB
INFO 01-23 11:02:31 distributed_gpu_executor.py:57] # GPU blocks: 82342, # CPU blocks: 5041
[rank0]:[E123 12:38:13.268138469 ProcessGroupNCCL.cpp:1375] [PG 5 Rank 0] First PG on this rank that detected no heartbeat of its watchdog.
[rank0]:[E123 12:40:05.402167831 ProcessGroupNCCL.cpp:1413] [PG 5 Rank 0] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=0
[rank0]:[E123 13:27:43.181427587 ProcessGroupNCCL.cpp:1442] Could not acquire GIL within 300 ms on exit, possible GIL induced hang
[rank0]:[F123 14:18:35.080933260 ProcessGroupNCCL.cpp:1224] [PG 5 Rank 0] [PG 5 Rank 0] ProcessGroupNCCL's watchdog got stuck for 600 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 0
*** SIGABRT received at time=1737659915 on cpu 32 ***
slurmstepd: error: *** JOB 413286 ON evc31 CANCELLED AT 2025-01-23T14:22:09 DUE TO NODE FAILURE, SEE SLURMCTLD LOG FOR DETAILS ***
slurmstepd: error: Detected 1 oom_kill event in StepId=413286.batch. Some of the step tasks have been OOM Killed.
slurmstepd: error: *** JOB 413286 STEPD TERMINATED ON evc31 AT 2025-01-23T14:24:14 DUE TO JOB NOT ENDING WITH SIGNALS ***
slurmstepd: error: Container 3172877 in cgroup plugin has 70 processes, giving up after 127 sec
