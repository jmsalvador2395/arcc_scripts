loaded anaconda
loaded cuda
activated conda environment
changed directory to /home/jsalvador/projects/llmeval
[[33mINFO[0m] running infill_solve using meta-llama/Llama-3.3-70B-Instruct
[[33mINFO[0m] fetching subsample IDs
counting prompt variations
[[33mINFO[0m] unique template_name query took 26.84 seconds
[[33mINFO[0m] unique template_id query took 27.34 seconds
[[33mINFO[0m] unique sys_id query took 27.33 seconds
fitb_l0, tids: 1, sids: 6
[[33mINFO[0m] unique template_id query took 27.11 seconds
[[33mINFO[0m] unique sys_id query took 27.05 seconds
fitb_l1, tids: 4, sids: 6
[[33mINFO[0m] unique template_id query took 27.30 seconds
[[33mINFO[0m] unique sys_id query took 27.10 seconds
fitb_l2, tids: 5, sids: 6
[[33mINFO[0m] unique template_id query took 27.23 seconds
[[33mINFO[0m] unique sys_id query took 27.35 seconds
fitb_l3, tids: 5, sids: 6
[[33mINFO[0m] unique template_id query took 27.49 seconds
[[33mINFO[0m] unique sys_id query took 27.59 seconds
fitb_l4, tids: 6, sids: 6
[[33mINFO[0m] prompts to process: 126000
creating generator function
[[33mINFO[0m] loading model: meta-llama/Llama-3.3-70B-Instruct
WARNING 01-18 09:30:18 config.py:319] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.
WARNING 01-18 09:30:18 arg_utils.py:930] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 01-18 09:30:18 config.py:1010] Chunked prefill is enabled with max_num_batched_tokens=512.
WARNING 01-18 09:30:18 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-18 09:30:27,815	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-18 09:30:29 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=50000, download_dir='/lustre/fs1/home/jsalvador/projects/llmeval/data/llm_cache', load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1737210618, served_model_name=meta-llama/Llama-3.3-70B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-18 09:30:30 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-18 09:31:06 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-18 09:31:06 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=560794)[0m INFO 01-18 09:31:06 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=560794)[0m INFO 01-18 09:31:06 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-18 09:31:06 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=560794)[0m INFO 01-18 09:31:06 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-18 09:31:06 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x148c26b943d0>, local_subscribe_port=40105, remote_subscribe_port=None)
INFO 01-18 09:31:06 model_runner.py:1014] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[36m(RayWorkerWrapper pid=560794)[0m INFO 01-18 09:31:06 model_runner.py:1014] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
INFO 01-18 09:31:07 loader.py:1014] Loading weights with BitsAndBytes quantization.  May take a while ...
[36m(RayWorkerWrapper pid=560794)[0m INFO 01-18 09:31:07 loader.py:1014] Loading weights with BitsAndBytes quantization.  May take a while ...
INFO 01-18 09:31:07 weight_utils.py:242] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   3% Completed | 1/30 [00:17<08:15, 17.08s/it]
Loading safetensors checkpoint shards:   7% Completed | 2/30 [00:33<07:42, 16.51s/it]
Loading safetensors checkpoint shards:  10% Completed | 3/30 [00:45<06:35, 14.67s/it]
Loading safetensors checkpoint shards:  13% Completed | 4/30 [00:57<05:51, 13.53s/it]
Loading safetensors checkpoint shards:  17% Completed | 5/30 [01:08<05:16, 12.64s/it]
Loading safetensors checkpoint shards:  20% Completed | 6/30 [01:20<04:59, 12.49s/it]
Loading safetensors checkpoint shards:  23% Completed | 7/30 [01:31<04:34, 11.96s/it]
Loading safetensors checkpoint shards:  27% Completed | 8/30 [01:47<04:49, 13.18s/it]
Loading safetensors checkpoint shards:  30% Completed | 9/30 [02:04<05:00, 14.29s/it]
Loading safetensors checkpoint shards:  33% Completed | 10/30 [02:15<04:27, 13.37s/it]
Loading safetensors checkpoint shards:  37% Completed | 11/30 [02:25<03:56, 12.46s/it]
Loading safetensors checkpoint shards:  40% Completed | 12/30 [02:38<03:47, 12.64s/it]
Loading safetensors checkpoint shards:  43% Completed | 13/30 [02:52<03:41, 13.02s/it]
Loading safetensors checkpoint shards:  47% Completed | 14/30 [03:07<03:37, 13.57s/it]
Loading safetensors checkpoint shards:  50% Completed | 15/30 [03:25<03:44, 14.97s/it]
Loading safetensors checkpoint shards:  53% Completed | 16/30 [03:36<03:12, 13.79s/it]
Loading safetensors checkpoint shards:  57% Completed | 17/30 [03:51<03:01, 13.99s/it]
Loading safetensors checkpoint shards:  60% Completed | 18/30 [04:04<02:43, 13.64s/it]
Loading safetensors checkpoint shards:  63% Completed | 19/30 [04:21<02:41, 14.67s/it]
Loading safetensors checkpoint shards:  67% Completed | 20/30 [04:34<02:21, 14.18s/it]
Loading safetensors checkpoint shards:  70% Completed | 21/30 [04:50<02:14, 14.91s/it]
Loading safetensors checkpoint shards:  73% Completed | 22/30 [05:03<01:53, 14.13s/it]
Loading safetensors checkpoint shards:  77% Completed | 23/30 [05:14<01:32, 13.16s/it]
Loading safetensors checkpoint shards:  80% Completed | 24/30 [05:27<01:19, 13.21s/it]
Loading safetensors checkpoint shards:  83% Completed | 25/30 [05:39<01:05, 13.03s/it]
Loading safetensors checkpoint shards:  87% Completed | 26/30 [05:52<00:51, 12.78s/it]
Loading safetensors checkpoint shards:  90% Completed | 27/30 [06:03<00:37, 12.39s/it]
Loading safetensors checkpoint shards:  93% Completed | 28/30 [06:17<00:25, 12.75s/it]
Loading safetensors checkpoint shards:  97% Completed | 29/30 [06:33<00:13, 13.89s/it]
Loading safetensors checkpoint shards: 100% Completed | 30/30 [06:46<00:00, 13.40s/it]
Loading safetensors checkpoint shards: 100% Completed | 30/30 [06:46<00:00, 13.54s/it]

[36m(RayWorkerWrapper pid=560794)[0m INFO 01-18 09:31:07 weight_utils.py:242] Using model weights format ['*.safetensors']
INFO 01-18 09:37:54 model_runner.py:1025] Loading model weights took 18.4395 GB
[36m(RayWorkerWrapper pid=560794)[0m INFO 01-18 09:37:54 model_runner.py:1025] Loading model weights took 18.4395 GB
INFO 01-18 09:37:56 distributed_gpu_executor.py:57] # GPU blocks: 20819, # CPU blocks: 1638
[[33mINFO[0m] begin collecting responses
generating responses:   0%|          | 0/378 [00:00<?, ?it/s]generating responses:   0%|          | 1/378 [22:19<140:15:18, 1339.31s/it]generating responses:   1%|          | 2/378 [40:09<123:20:50, 1180.99s/it]generating responses:   1%|          | 3/378 [59:43<122:41:16, 1177.81s/it]generating responses:   1%|          | 4/378 [1:18:21<119:55:09, 1154.30s/it]generating responses:   1%|▏         | 5/378 [1:38:45<122:11:07, 1179.27s/it]generating responses:   2%|▏         | 6/378 [2:01:10<127:42:09, 1235.83s/it]generating responses:   2%|▏         | 7/378 [2:22:10<128:09:36, 1243.60s/it]generating responses:   2%|▏         | 8/378 [2:42:24<126:50:05, 1234.07s/it]generating responses:   2%|▏         | 9/378 [3:03:05<126:43:22, 1236.32s/it]generating responses:   3%|▎         | 10/378 [3:21:59<123:09:12, 1204.76s/it]generating responses:   3%|▎         | 11/378 [3:43:15<125:03:10, 1226.68s/it]generating responses:   3%|▎         | 12/378 [4:03:48<124:52:54, 1228.35s/it]generating responses:   3%|▎         | 13/378 [4:20:51<118:15:07, 1166.32s/it]generating responses:   4%|▎         | 14/378 [4:42:35<122:07:41, 1207.86s/it]generating responses:   4%|▍         | 15/378 [5:01:16<119:09:24, 1181.72s/it]generating responses:   4%|▍         | 16/378 [5:26:04<128:05:54, 1273.91s/it]generating responses:   4%|▍         | 17/378 [5:50:29<133:30:56, 1331.46s/it]generating responses:   5%|▍         | 18/378 [6:14:33<136:30:18, 1365.05s/it]generating responses:   5%|▌         | 19/378 [6:35:11<132:20:13, 1327.06s/it]generating responses:   5%|▌         | 20/378 [6:56:27<130:25:56, 1311.61s/it]generating responses:   6%|▌         | 21/378 [7:17:40<128:55:08, 1300.02s/it]generating responses:   6%|▌         | 22/378 [7:36:41<123:50:34, 1252.34s/it]generating responses:   6%|▌         | 23/378 [7:59:09<126:19:30, 1281.04s/it]generating responses:   6%|▋         | 24/378 [8:18:04<121:39:10, 1237.15s/it]generating responses:   7%|▋         | 25/378 [8:39:30<122:44:56, 1251.83s/it]generating responses:   7%|▋         | 26/378 [9:01:34<124:32:00, 1273.64s/it]generating responses:   7%|▋         | 27/378 [9:23:17<125:02:21, 1282.45s/it]generating responses:   7%|▋         | 28/378 [9:45:05<125:24:36, 1289.93s/it]generating responses:   8%|▊         | 29/378 [10:06:09<124:18:45, 1282.31s/it]generating responses:   8%|▊         | 30/378 [10:28:00<124:47:03, 1290.87s/it]generating responses:   8%|▊         | 31/378 [10:50:11<125:35:38, 1302.99s/it]generating responses:   8%|▊         | 32/378 [11:11:47<125:00:26, 1300.66s/it]generating responses:   9%|▊         | 33/378 [11:33:39<124:59:44, 1304.30s/it]generating responses:   9%|▉         | 34/378 [11:57:03<127:29:34, 1334.23s/it]generating responses:   9%|▉         | 35/378 [12:19:26<127:22:15, 1336.84s/it]generating responses:  10%|▉         | 36/378 [12:42:43<128:41:18, 1354.62s/it]generating responses:  10%|▉         | 37/378 [13:00:44<120:32:51, 1272.64s/it]generating responses:  10%|█         | 38/378 [13:15:48<109:44:51, 1162.04s/it]generating responses:  10%|█         | 39/378 [13:28:46<98:35:38, 1047.02s/it] generating responses:  11%|█         | 40/378 [13:47:43<100:49:17, 1073.84s/it]generating responses:  11%|█         | 41/378 [14:03:25<96:49:06, 1034.26s/it] generating responses:  11%|█         | 42/378 [14:21:40<98:14:56, 1052.67s/it]generating responses:  11%|█▏        | 43/378 [14:38:38<96:57:58, 1042.03s/it]generating responses:  12%|█▏        | 44/378 [14:55:27<95:46:09, 1032.24s/it]generating responses:  12%|█▏        | 45/378 [15:13:20<96:36:49, 1044.47s/it]generating responses:  12%|█▏        | 46/378 [15:31:06<96:55:49, 1051.05s/it]generating responses:  12%|█▏        | 47/378 [15:47:38<94:59:59, 1033.23s/it]generating responses:  13%|█▎        | 48/378 [16:06:48<97:55:43, 1068.31s/it]generating responses:  13%|█▎        | 49/378 [16:28:36<104:11:58, 1140.18s/it]generating responses:  13%|█▎        | 50/378 [16:48:02<104:34:42, 1147.81s/it]generating responses:  13%|█▎        | 51/378 [17:05:15<101:08:38, 1113.51s/it]generating responses:  14%|█▍        | 52/378 [17:23:43<100:40:52, 1111.82s/it]generating responses:  14%|█▍        | 53/378 [17:47:04<108:11:30, 1198.43s/it]generating responses:  14%|█▍        | 54/378 [18:09:43<112:12:25, 1246.74s/it]generating responses:  15%|█▍        | 55/378 [18:20:10<95:10:10, 1060.71s/it] generating responses:  15%|█▍        | 56/378 [18:30:27<82:58:28, 927.67s/it] generating responses:  15%|█▌        | 57/378 [18:43:41<79:08:47, 887.63s/it]generating responses:  15%|█▌        | 58/378 [18:54:07<71:54:41, 809.01s/it]generating responses:  16%|█▌        | 59/378 [19:04:22<66:32:03, 750.86s/it]generating responses:  16%|█▌        | 60/378 [19:14:36<62:41:24, 709.70s/it]generating responses:  16%|█▌        | 61/378 [19:25:25<60:53:29, 691.51s/it]generating responses:  16%|█▋        | 62/378 [19:35:34<58:32:40, 666.96s/it]generating responses:  17%|█▋        | 63/378 [19:46:08<57:29:14, 657.00s/it]generating responses:  17%|█▋        | 64/378 [19:56:42<56:41:54, 650.05s/it]generating responses:  17%|█▋        | 65/378 [20:06:56<55:35:11, 639.34s/it]generating responses:  17%|█▋        | 66/378 [20:17:26<55:10:10, 636.57s/it]generating responses:  18%|█▊        | 67/378 [20:27:50<54:38:57, 632.60s/it]generating responses:  18%|█▊        | 68/378 [20:38:00<53:54:13, 625.98s/it]generating responses:  18%|█▊        | 69/378 [20:48:36<53:59:26, 629.02s/it]generating responses:  19%|█▊        | 70/378 [21:00:06<55:21:59, 647.14s/it]generating responses:  19%|█▉        | 71/378 [21:11:33<56:12:07, 659.05s/it]generating responses:  19%|█▉        | 72/378 [21:23:12<57:02:48, 671.14s/it]generating responses:  19%|█▉        | 73/378 [21:43:06<70:08:34, 827.92s/it]generating responses:  20%|█▉        | 74/378 [22:04:54<82:05:23, 972.12s/it]generating responses:  20%|█▉        | 75/378 [22:25:26<88:22:50, 1050.07s/it]generating responses:  20%|██        | 76/378 [22:46:15<93:05:00, 1109.60s/it]generating responses:  20%|██        | 77/378 [23:06:23<95:15:14, 1139.25s/it]generating responses:  21%|██        | 78/378 [23:26:58<97:19:14, 1167.85s/it]generating responses:  21%|██        | 79/378 [23:46:01<96:22:39, 1160.40s/it]generating responses:  21%|██        | 80/378 [24:07:45<99:37:29, 1203.52s/it]generating responses:  21%|██▏       | 81/378 [24:29:30<101:48:49, 1234.11s/it]generating responses:  22%|██▏       | 82/378 [24:51:36<103:44:12, 1261.66s/it]generating responses:  22%|██▏       | 83/378 [25:14:21<105:55:42, 1292.69s/it]generating responses:  22%|██▏       | 84/378 [25:36:07<105:52:29, 1296.43s/it]generating responses:  22%|██▏       | 85/378 [25:55:54<102:51:33, 1263.80s/it]generating responses:  23%|██▎       | 86/378 [26:17:56<103:54:27, 1281.05s/it]generating responses:  23%|██▎       | 87/378 [26:39:13<103:28:06, 1280.02s/it]generating responses:  23%|██▎       | 88/378 [27:01:38<104:40:38, 1299.44s/it]generating responses:  24%|██▎       | 89/378 [27:21:30<101:43:28, 1267.16s/it]generating responses:  24%|██▍       | 90/378 [27:44:11<103:37:31, 1295.32s/it]generating responses:  24%|██▍       | 91/378 [27:54:39<87:18:02, 1095.06s/it] generating responses:  24%|██▍       | 92/378 [28:04:58<75:39:27, 952.34s/it] generating responses:  25%|██▍       | 93/378 [28:15:11<67:19:54, 850.51s/it]generating responses:  25%|██▍       | 94/378 [28:25:31<61:38:15, 781.32s/it]generating responses:  25%|██▌       | 95/378 [28:35:41<57:23:36, 730.09s/it]generating responses:  25%|██▌       | 96/378 [28:45:52<54:23:43, 694.41s/it]generating responses:  26%|██▌       | 97/378 [28:56:09<52:23:00, 671.10s/it]generating responses:  26%|██▌       | 98/378 [29:11:04<57:24:31, 738.11s/it]generating responses:  26%|██▌       | 99/378 [29:21:25<54:29:00, 703.01s/it]generating responses:  26%|██▋       | 100/378 [29:31:40<52:14:50, 676.59s/it]generating responses:  27%|██▋       | 101/378 [29:41:45<50:24:52, 655.21s/it]generating responses:  27%|██▋       | 102/378 [29:51:49<49:03:22, 639.87s/it]generating responses:  27%|██▋       | 103/378 [30:02:02<48:15:25, 631.73s/it]generating responses:  28%|██▊       | 104/378 [30:12:11<47:34:01, 624.97s/it]generating responses:  28%|██▊       | 105/378 [30:22:25<47:08:12, 621.59s/it]generating responses:  28%|██▊       | 106/378 [30:32:54<47:08:40, 623.97s/it]generating responses:  28%|██▊       | 107/378 [30:43:13<46:50:46, 622.31s/it]generating responses:  29%|██▊       | 108/378 [30:53:31<46:35:35, 621.24s/it]generating responses:  29%|██▉       | 109/378 [31:07:07<50:46:23, 679.49s/it]generating responses:  29%|██▉       | 110/378 [31:17:22<49:09:21, 660.30s/it]generating responses:  29%|██▉       | 111/378 [31:27:40<48:01:02, 647.43s/it]generating responses:  30%|██▉       | 112/378 [31:42:47<53:36:01, 725.42s/it]generating responses:  30%|██▉       | 113/378 [31:53:09<51:07:20, 694.49s/it]generating responses:  30%|███       | 114/378 [32:03:27<49:13:53, 671.34s/it]generating responses:  30%|███       | 115/378 [32:13:39<47:45:37, 653.75s/it]generating responses:  31%|███       | 116/378 [32:23:49<46:36:47, 640.49s/it]generating responses:  31%|███       | 117/378 [32:33:59<45:46:18, 631.33s/it]generating responses:  31%|███       | 118/378 [32:44:22<45:25:37, 628.99s/it]generating responses:  31%|███▏      | 119/378 [32:54:58<45:23:44, 630.98s/it]generating responses:  32%|███▏      | 120/378 [33:05:29<45:12:33, 630.83s/it]generating responses:  32%|███▏      | 121/378 [33:16:03<45:06:59, 631.98s/it]generating responses:  32%|███▏      | 122/378 [33:30:42<50:12:33, 706.07s/it]generating responses:  33%|███▎      | 123/378 [33:41:22<48:36:52, 686.33s/it]generating responses:  33%|███▎      | 124/378 [33:52:35<48:07:53, 682.18s/it]generating responses:  33%|███▎      | 125/378 [34:06:27<51:06:20, 727.20s/it]generating responses:  33%|███▎      | 126/378 [34:17:52<50:00:36, 714.43s/it]generating responses:  34%|███▎      | 127/378 [34:40:07<62:47:49, 900.67s/it]generating responses:  34%|███▍      | 128/378 [35:03:04<72:28:11, 1043.57s/it]generating responses:  34%|███▍      | 129/378 [35:24:44<77:29:40, 1120.40s/it]generating responses:  34%|███▍      | 130/378 [35:47:34<82:20:14, 1195.22s/it]generating responses:  35%|███▍      | 131/378 [36:09:27<84:26:50, 1230.81s/it]generating responses:  35%|███▍      | 132/378 [36:32:28<87:10:33, 1275.74s/it]generating responses:  35%|███▌      | 133/378 [36:53:55<87:02:36, 1279.00s/it]generating responses:  35%|███▌      | 134/378 [37:16:47<88:34:50, 1306.93s/it]generating responses:  36%|███▌      | 135/378 [37:39:59<89:56:31, 1332.47s/it]generating responses:  36%|███▌      | 136/378 [38:04:16<92:05:38, 1370.00s/it]generating responses:  36%|███▌      | 137/378 [38:26:16<90:42:36, 1355.01s/it]generating responses:  37%|███▋      | 138/378 [38:49:07<90:39:21, 1359.84s/it]generating responses:  37%|███▋      | 139/378 [39:10:34<88:48:54, 1337.80s/it]generating responses:  37%|███▋      | 140/378 [39:32:14<87:41:48, 1326.51s/it]slurmstepd: error: *** JOB 412008 ON evc27 CANCELLED AT 2025-01-20T01:24:47 DUE TO TIME LIMIT ***
