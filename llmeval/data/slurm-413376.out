loaded anaconda
loaded cuda
activated conda environment
changed directory to /home/jsalvador/projects/llmeval
------------------
*** EVALUATING allenai/OLMo-2-1124-7B-Instruct ***
WARNING 01-23 15:27:03 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-23 15:27:17,303	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-23 15:27:35 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-7B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-7B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-23 15:27:36 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-23 15:28:14 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-23 15:28:14 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=815267)[0m INFO 01-23 15:28:14 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=815267)[0m INFO 01-23 15:28:14 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-23 15:28:16 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=815267)[0m INFO 01-23 15:28:16 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 15:28:16 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x1515b12dbf90>, local_subscribe_port=55503, remote_subscribe_port=None)
INFO 01-23 15:28:16 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
[36m(RayWorkerWrapper pid=815267)[0m INFO 01-23 15:28:16 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
ERROR 01-23 15:28:16 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-23 15:28:16 worker_base.py:464] Traceback (most recent call last):
ERROR 01-23 15:28:16 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-23 15:28:16 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-23 15:28:16 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:28:16 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-23 15:28:16 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-23 15:28:16 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-23 15:28:16 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-23 15:28:16 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:28:16 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-23 15:28:16 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-23 15:28:16 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:28:16 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-23 15:28:16 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-23 15:28:16 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:28:16 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-23 15:28:16 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-23 15:28:16 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:28:16 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-23 15:28:16 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-23 15:28:16 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:28:16 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-23 15:28:16 worker_base.py:464]     raise ValueError(
ERROR 01-23 15:28:16 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-7B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-23 15:28:17,034	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=815267, ip=172.17.11.38, actor_id=cdd8664360ce68c8c06b4cf901000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x143a84849a50>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=815267)[0m ERROR 01-23 15:28:16 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W123 15:28:21.352948840 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
WARNING 01-23 15:28:42 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-23 15:28:52,266	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-23 15:29:10 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-7B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-7B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-23 15:29:10 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-23 15:29:47 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-23 15:29:47 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=819687)[0m INFO 01-23 15:29:47 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=819687)[0m INFO 01-23 15:29:47 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-23 15:29:47 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 15:29:47 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x14c43c48ba90>, local_subscribe_port=36817, remote_subscribe_port=None)
INFO 01-23 15:29:47 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
[36m(RayWorkerWrapper pid=819687)[0m INFO 01-23 15:29:47 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=819687)[0m INFO 01-23 15:29:47 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
ERROR 01-23 15:29:47 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-23 15:29:47 worker_base.py:464] Traceback (most recent call last):
ERROR 01-23 15:29:47 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-23 15:29:47 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-23 15:29:47 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:29:47 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-23 15:29:47 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-23 15:29:47 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-23 15:29:47 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-23 15:29:47 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:29:47 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-23 15:29:47 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-23 15:29:47 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:29:47 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-23 15:29:47 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-23 15:29:47 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:29:47 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-23 15:29:47 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-23 15:29:47 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:29:47 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-23 15:29:47 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-23 15:29:47 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:29:47 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-23 15:29:47 worker_base.py:464]     raise ValueError(
ERROR 01-23 15:29:47 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-7B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-23 15:29:48,014	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=819687, ip=172.17.11.38, actor_id=4690713b88f6f55430082cea01000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x145badb9b1d0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=819687)[0m ERROR 01-23 15:29:47 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W123 15:29:51.210953981 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
WARNING 01-23 15:30:04 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-23 15:30:14,986	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-23 15:30:32 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-7B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-7B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-23 15:30:33 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-23 15:31:09 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-23 15:31:09 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=824138)[0m INFO 01-23 15:31:09 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=824138)[0m INFO 01-23 15:31:09 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-23 15:31:10 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 15:31:10 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x151d04c2cc50>, local_subscribe_port=38105, remote_subscribe_port=None)
INFO 01-23 15:31:10 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
[36m(RayWorkerWrapper pid=824138)[0m INFO 01-23 15:31:10 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=824138)[0m INFO 01-23 15:31:10 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
ERROR 01-23 15:31:10 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-23 15:31:10 worker_base.py:464] Traceback (most recent call last):
ERROR 01-23 15:31:10 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-23 15:31:10 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-23 15:31:10 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:31:10 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-23 15:31:10 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-23 15:31:10 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-23 15:31:10 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-23 15:31:10 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:31:10 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-23 15:31:10 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-23 15:31:10 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:31:10 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-23 15:31:10 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-23 15:31:10 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:31:10 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-23 15:31:10 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-23 15:31:10 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:31:10 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-23 15:31:10 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-23 15:31:10 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:31:10 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-23 15:31:10 worker_base.py:464]     raise ValueError(
ERROR 01-23 15:31:10 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-7B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-23 15:31:10,868	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=824138, ip=172.17.11.38, actor_id=3c154660777eb4127b79f94b01000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x143033924610>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=824138)[0m ERROR 01-23 15:31:10 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W123 15:31:14.142833590 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
WARNING 01-23 15:31:27 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-23 15:31:37,997	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-23 15:31:56 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-7B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-7B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-23 15:31:56 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-23 15:32:32 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-23 15:32:32 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=828567)[0m INFO 01-23 15:32:32 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=828567)[0m INFO 01-23 15:32:32 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-23 15:32:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 15:32:33 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x151fef077d50>, local_subscribe_port=45477, remote_subscribe_port=None)
INFO 01-23 15:32:33 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
[36m(RayWorkerWrapper pid=828567)[0m INFO 01-23 15:32:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=828567)[0m INFO 01-23 15:32:33 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-7B-Instruct...
ERROR 01-23 15:32:33 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-23 15:32:33 worker_base.py:464] Traceback (most recent call last):
ERROR 01-23 15:32:33 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-23 15:32:33 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-23 15:32:33 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:32:33 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-23 15:32:33 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-23 15:32:33 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-23 15:32:33 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-23 15:32:33 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:32:33 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-23 15:32:33 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-23 15:32:33 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:32:33 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-23 15:32:33 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-23 15:32:33 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:32:33 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-23 15:32:33 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-23 15:32:33 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:32:33 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-23 15:32:33 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-23 15:32:33 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:32:33 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-23 15:32:33 worker_base.py:464]     raise ValueError(
ERROR 01-23 15:32:33 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-7B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-23 15:32:33,633	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=828567, ip=172.17.11.38, actor_id=1c6ba81434804c35e8d5432901000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x14b9dfd36250>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=828567)[0m ERROR 01-23 15:32:33 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W123 15:32:36.566919282 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
*** EVALUATING allenai/OLMo-2-1124-13B-Instruct ***
WARNING 01-23 15:32:50 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-23 15:33:00,530	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-23 15:33:18 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-13B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-13B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-13B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-23 15:33:19 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-23 15:33:54 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-23 15:33:54 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=832989)[0m INFO 01-23 15:33:54 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=832989)[0m INFO 01-23 15:33:54 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-23 15:33:54 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 15:33:54 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x1504831b5fd0>, local_subscribe_port=58225, remote_subscribe_port=None)
[36m(RayWorkerWrapper pid=832989)[0m INFO 01-23 15:33:54 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 15:33:54 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
[36m(RayWorkerWrapper pid=832989)[0m INFO 01-23 15:33:54 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
ERROR 01-23 15:33:55 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-23 15:33:55 worker_base.py:464] Traceback (most recent call last):
ERROR 01-23 15:33:55 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-23 15:33:55 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-23 15:33:55 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:33:55 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-23 15:33:55 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-23 15:33:55 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-23 15:33:55 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-23 15:33:55 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:33:55 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-23 15:33:55 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-23 15:33:55 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:33:55 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-23 15:33:55 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-23 15:33:55 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:33:55 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-23 15:33:55 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-23 15:33:55 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:33:55 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-23 15:33:55 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-23 15:33:55 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:33:55 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-23 15:33:55 worker_base.py:464]     raise ValueError(
ERROR 01-23 15:33:55 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-13B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-23 15:33:55,151	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=832989, ip=172.17.11.38, actor_id=a5fef186f00446a65738f62501000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x15139f4056d0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=832989)[0m ERROR 01-23 15:33:55 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W123 15:33:58.333421704 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
WARNING 01-23 15:34:11 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-23 15:34:22,194	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-23 15:34:40 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-13B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-13B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-13B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-23 15:34:40 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-23 15:35:17 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-23 15:35:17 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=837423)[0m INFO 01-23 15:35:17 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=837423)[0m INFO 01-23 15:35:17 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-23 15:35:18 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 15:35:18 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x14dca7077b90>, local_subscribe_port=43851, remote_subscribe_port=None)
INFO 01-23 15:35:18 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
[36m(RayWorkerWrapper pid=837423)[0m INFO 01-23 15:35:18 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=837423)[0m INFO 01-23 15:35:18 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
ERROR 01-23 15:35:18 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-23 15:35:18 worker_base.py:464] Traceback (most recent call last):
ERROR 01-23 15:35:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-23 15:35:18 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-23 15:35:18 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:35:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-23 15:35:18 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-23 15:35:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-23 15:35:18 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-23 15:35:18 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:35:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-23 15:35:18 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-23 15:35:18 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:35:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-23 15:35:18 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-23 15:35:18 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:35:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-23 15:35:18 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-23 15:35:18 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:35:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-23 15:35:18 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-23 15:35:18 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:35:18 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-23 15:35:18 worker_base.py:464]     raise ValueError(
ERROR 01-23 15:35:18 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-13B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-23 15:35:19,060	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=837423, ip=172.17.11.38, actor_id=9b92975571d5bf1d132dec7001000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x14dd288c2e90>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=837423)[0m ERROR 01-23 15:35:19 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W123 15:35:22.209915096 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
WARNING 01-23 15:35:36 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-23 15:35:46,354	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-23 15:36:04 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-13B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-13B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-13B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-23 15:36:04 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-23 15:36:41 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-23 15:36:41 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=841825)[0m INFO 01-23 15:36:41 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=841825)[0m INFO 01-23 15:36:41 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-23 15:36:41 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 15:36:41 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x153728890c10>, local_subscribe_port=60307, remote_subscribe_port=None)
INFO 01-23 15:36:41 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
ERROR 01-23 15:36:41 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-23 15:36:41 worker_base.py:464] Traceback (most recent call last):
ERROR 01-23 15:36:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-23 15:36:41 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-23 15:36:41 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:36:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-23 15:36:41 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-23 15:36:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-23 15:36:41 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-23 15:36:41 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:36:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-23 15:36:41 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-23 15:36:41 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:36:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-23 15:36:41 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-23 15:36:41 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:36:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-23 15:36:41 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-23 15:36:41 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:36:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-23 15:36:41 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-23 15:36:41 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:36:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-23 15:36:41 worker_base.py:464]     raise ValueError(
ERROR 01-23 15:36:41 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-13B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-23 15:36:41,995	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=841825, ip=172.17.11.38, actor_id=a398e27b2e2bcf271d45200901000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x14c9ea148550>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=841825)[0m INFO 01-23 15:36:41 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=841825)[0m INFO 01-23 15:36:41 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=841825)[0m ERROR 01-23 15:36:41 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W123 15:36:45.166737175 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
WARNING 01-23 15:36:58 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-23 15:37:08,914	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-23 15:37:26 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='allenai/OLMo-2-1124-13B-Instruct', speculative_config=None, tokenizer='allenai/OLMo-2-1124-13B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMo-2-1124-13B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-23 15:37:27 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-23 15:38:01 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-23 15:38:01 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=846250)[0m INFO 01-23 15:38:01 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=846250)[0m INFO 01-23 15:38:01 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-23 15:38:01 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 15:38:01 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x150cc7c83f50>, local_subscribe_port=52201, remote_subscribe_port=None)
[36m(RayWorkerWrapper pid=846250)[0m INFO 01-23 15:38:01 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 15:38:01 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
ERROR 01-23 15:38:02 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
ERROR 01-23 15:38:02 worker_base.py:464] Traceback (most recent call last):
ERROR 01-23 15:38:02 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
ERROR 01-23 15:38:02 worker_base.py:464]     return executor(*args, **kwargs)
ERROR 01-23 15:38:02 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:38:02 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 01-23 15:38:02 worker_base.py:464]     self.model_runner.load_model()
ERROR 01-23 15:38:02 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
ERROR 01-23 15:38:02 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
ERROR 01-23 15:38:02 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:38:02 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
ERROR 01-23 15:38:02 worker_base.py:464]     return loader.load_model(model_config=model_config,
ERROR 01-23 15:38:02 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:38:02 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
ERROR 01-23 15:38:02 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
ERROR 01-23 15:38:02 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:38:02 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
ERROR 01-23 15:38:02 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
ERROR 01-23 15:38:02 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:38:02 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
ERROR 01-23 15:38:02 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
ERROR 01-23 15:38:02 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-23 15:38:02 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
ERROR 01-23 15:38:02 worker_base.py:464]     raise ValueError(
ERROR 01-23 15:38:02 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
ERROR: FAILED TO LOAD MODEL allenai/OLMo-2-1124-13B-Instruct
Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
2025-01-23 15:38:02,102	ERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::RayWorkerWrapper.execute_method()[39m (pid=846250, ip=172.17.11.38, actor_id=305e0a009218a0efb75719a801000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x151787852690>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 465, in execute_method
    raise e
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
    return executor(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
    self.model = get_model(model_config=self.model_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
    return loader.load_model(model_config=model_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
    model = _initialize_model(model_config, self.load_config,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
    model_class, _ = get_model_architecture(model_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
    return ModelRegistry.resolve_model_cls(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
    raise ValueError(
ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[36m(RayWorkerWrapper pid=846250)[0m INFO 01-23 15:38:01 model_runner.py:1014] Starting to load model allenai/OLMo-2-1124-13B-Instruct...
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464] Error executing method load_model. This might cause deadlock in distributed execution.
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464] Traceback (most recent call last):
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 456, in execute_method
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]     return executor(*args, **kwargs)
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/worker.py", line 183, in load_model
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]     self.model_runner.load_model()
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/worker/model_runner.py", line 1016, in load_model
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]     self.model = get_model(model_config=self.model_config,
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 19, in get_model
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]     return loader.load_model(model_config=model_config,
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 399, in load_model
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]     model = _initialize_model(model_config, self.load_config,
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 174, in _initialize_model
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]     model_class, _ = get_model_architecture(model_config)
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/model_loader/utils.py", line 33, in get_model_architecture
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]     return ModelRegistry.resolve_model_cls(architectures)
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]   File "/home/jsalvador/.conda/envs/llmeval/lib/python3.11/site-packages/vllm/model_executor/models/__init__.py", line 185, in resolve_model_cls
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464]     raise ValueError(
[36m(RayWorkerWrapper pid=846250)[0m ERROR 01-23 15:38:02 worker_base.py:464] ValueError: Model architectures ['Olmo2ForCausalLM'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'MllamaForConditionalGeneration', 'BartModel', 'BartForConditionalGeneration']
[rank0]:[W123 15:38:04.070368828 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
/home/jsalvador/.conda/envs/llmeval/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
*** EVALUATING google/gemma-2-2b-it ***
WARNING 01-23 15:38:18 utils.py:747] Gemma 2 uses sliding window attention for every odd layer, which is currently not supported by vLLM. Disabling sliding window and capping the max length to the sliding window size (4096).
WARNING 01-23 15:38:18 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-23 15:38:28,884	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-23 15:38:46 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='google/gemma-2-2b-it', speculative_config=None, tokenizer='google/gemma-2-2b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/home/jsalvador/llm_cache', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2-2b-it, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-23 15:38:48 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-23 15:39:24 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-23 15:39:24 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=850710)[0m INFO 01-23 15:39:24 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=850710)[0m INFO 01-23 15:39:24 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-23 15:39:25 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-23 15:39:25 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x1484b36ae010>, local_subscribe_port=43839, remote_subscribe_port=None)
INFO 01-23 15:39:25 model_runner.py:1014] Starting to load model google/gemma-2-2b-it...
[36m(RayWorkerWrapper pid=850710)[0m INFO 01-23 15:39:25 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=850710)[0m INFO 01-23 15:39:25 model_runner.py:1014] Starting to load model google/gemma-2-2b-it...
INFO 01-23 15:39:27 weight_utils.py:242] Using model weights format ['*.safetensors']
[36m(RayWorkerWrapper pid=850710)[0m INFO 01-23 15:39:27 weight_utils.py:242] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  6.15it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.16it/s]

INFO 01-23 15:41:30 model_runner.py:1025] Loading model weights took 2.4389 GB
[36m(RayWorkerWrapper pid=850710)[0m INFO 01-23 15:41:30 model_runner.py:1025] Loading model weights took 2.4389 GB
INFO 01-23 15:41:47 distributed_gpu_executor.py:57] # GPU blocks: 82342, # CPU blocks: 5041
slurmstepd: error: *** JOB 413376 ON evc38 CANCELLED AT 2025-01-23T19:10:18 DUE TO NODE FAILURE, SEE SLURMCTLD LOG FOR DETAILS ***
slurmstepd: error: Detected 2 oom_kill events in StepId=413376.batch. Some of the step tasks have been OOM Killed.
