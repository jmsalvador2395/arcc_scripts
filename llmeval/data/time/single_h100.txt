loaded anaconda
loaded cuda
activated conda environment
changed directory to /home/jsalvador/projects/llmeval
------------------
WARNING 01-15 22:31:29 arg_utils.py:930] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 01-15 22:31:29 config.py:1010] Chunked prefill is enabled with max_num_batched_tokens=512.
WARNING 01-15 22:31:29 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-15 22:31:47,180	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-15 22:31:50 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=50000, download_dir='data/llm_cache/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-15 22:31:51 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-15 22:32:14 model_runner.py:1014] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 01-15 22:32:19 weight_utils.py:242] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:05<00:15,  5.32s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:11<00:11,  5.55s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:12<00:03,  3.85s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:19<00:00,  5.06s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:19<00:00,  4.94s/it]

INFO 01-15 22:32:39 model_runner.py:1025] Loading model weights took 14.9888 GB
INFO 01-15 22:32:47 distributed_gpu_executor.py:57] # GPU blocks: 28047, # CPU blocks: 2048
0it [00:00, ?it/s]1it [00:15, 15.89s/it]2it [00:26, 12.87s/it]3it [12:31, 338.10s/it]4it [12:42, 209.04s/it]5it [12:55, 138.22s/it]6it [25:07, 339.96s/it]7it [42:26, 568.72s/it]8it [1:01:37, 753.82s/it]9it [1:18:45, 839.74s/it]10it [1:18:58, 584.31s/it]10it [1:18:58, 473.81s/it]
*********************************************
model: meta-llama/Llama-3.1-8B-Instruct, limit: 10000, num_devices: 1, time: 4738.05300784111
*********************************************
WARNING 01-15 23:52:31 arg_utils.py:930] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 01-15 23:52:31 config.py:1010] Chunked prefill is enabled with max_num_batched_tokens=512.
WARNING 01-15 23:52:31 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-15 23:52:40,092	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-15 23:52:42 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=50000, download_dir='data/llm_cache/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-1B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-15 23:52:43 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-15 23:53:01 model_runner.py:1014] Starting to load model meta-llama/Llama-3.2-1B-Instruct...
INFO 01-15 23:53:01 weight_utils.py:242] Using model weights format ['*.safetensors']
INFO 01-15 23:54:00 weight_utils.py:287] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.05s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.05s/it]

INFO 01-15 23:54:01 model_runner.py:1025] Loading model weights took 2.3185 GB
INFO 01-15 23:54:03 distributed_gpu_executor.py:57] # GPU blocks: 138190, # CPU blocks: 8192
0it [00:00, ?it/s]1it [00:06,  6.70s/it]2it [00:12,  6.17s/it]3it [00:17,  5.62s/it]4it [00:22,  5.51s/it]5it [00:28,  5.57s/it]6it [00:34,  5.60s/it]7it [00:39,  5.58s/it]8it [00:45,  5.53s/it]9it [00:50,  5.54s/it]10it [00:56,  5.52s/it]10it [00:56,  5.61s/it]
*********************************************
model: meta-llama/Llama-3.2-1B-Instruct, limit: 10000, num_devices: 1, time: 56.14432191848755
*********************************************
WARNING 01-15 23:55:18 arg_utils.py:930] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 01-15 23:55:18 config.py:1010] Chunked prefill is enabled with max_num_batched_tokens=512.
WARNING 01-15 23:55:18 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-15 23:55:26,255	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-15 23:55:28 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=50000, download_dir='data/llm_cache/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-15 23:55:29 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-15 23:55:47 model_runner.py:1014] Starting to load model meta-llama/Llama-3.2-3B-Instruct...
INFO 01-15 23:55:47 weight_utils.py:242] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.48s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  3.10s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.86s/it]

INFO 01-15 23:55:53 model_runner.py:1025] Loading model weights took 6.0160 GB
INFO 01-15 23:55:54 distributed_gpu_executor.py:57] # GPU blocks: 37321, # CPU blocks: 2340
0it [00:00, ?it/s]1it [01:52, 112.34s/it]2it [02:05, 54.00s/it] 3it [02:14, 33.32s/it]4it [12:58, 274.63s/it]5it [13:08, 179.25s/it]6it [23:51, 336.78s/it]7it [24:01, 229.82s/it]8it [24:13, 160.68s/it]9it [36:07, 333.54s/it]10it [47:54, 448.94s/it]10it [47:54, 287.47s/it]
*********************************************
model: meta-llama/Llama-3.2-3B-Instruct, limit: 10000, num_devices: 1, time: 2874.705672979355
*********************************************
