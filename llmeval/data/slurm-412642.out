loaded anaconda
loaded cuda
activated conda environment
changed directory to /home/jsalvador/projects/llmeval
[[33mINFO[0m] running infill_solve using meta-llama/Llama-3.3-70B-Instruct
[[33mINFO[0m] fetching subsample IDs
counting prompt variations
[[33mINFO[0m] unique template_name query took 2029.37 seconds
[[33mINFO[0m] unique template_id query took 2104.21 seconds
[[33mINFO[0m] unique sys_id query took 2071.31 seconds
fitb_l0, tids: 1, sids: 6
[[33mINFO[0m] unique template_id query took 2090.96 seconds
[[33mINFO[0m] unique sys_id query took 2113.10 seconds
fitb_l1, tids: 4, sids: 6
[[33mINFO[0m] unique template_id query took 2078.49 seconds
[[33mINFO[0m] unique sys_id query took 2138.44 seconds
fitb_l2, tids: 5, sids: 6
[[33mINFO[0m] unique template_id query took 2118.52 seconds
[[33mINFO[0m] unique sys_id query took 1747.71 seconds
fitb_l3, tids: 5, sids: 6
[[33mINFO[0m] unique template_id query took 453.56 seconds
[[33mINFO[0m] unique sys_id query took 27.08 seconds
fitb_l4, tids: 6, sids: 6
[[33mINFO[0m] prompts to process: 126000
creating generator function
[[33mINFO[0m] loading model: meta-llama/Llama-3.3-70B-Instruct
WARNING 01-21 05:32:48 config.py:319] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.
WARNING 01-21 05:32:48 arg_utils.py:930] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 01-21 05:32:48 config.py:1010] Chunked prefill is enabled with max_num_batched_tokens=512.
WARNING 01-21 05:32:48 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-01-21 05:33:13,861	ERROR services.py:1350 -- Failed to start the dashboard 
2025-01-21 05:33:13,862	ERROR services.py:1375 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure' to find where the log file is.
2025-01-21 05:33:13,863	ERROR services.py:1419 -- 
The last 20 lines of /tmp/ray/session_2025-01-21_05-32-48_930550_3768674/logs/dashboard.log (it contains the error message from the dashboard): 
2025-01-21 05:33:22,088	INFO worker.py:1821 -- Started a local Ray instance.
INFO 01-21 05:33:34 ray_utils.py:183] Waiting for creating a placement group of specs for 10 seconds. specs=[{'node:172.17.11.35': 0.001, 'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` to see if you have enough resources.
INFO 01-21 05:33:39 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='meta-llama/Llama-3.3-70B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.3-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=50000, download_dir='/lustre/fs1/home/jsalvador/projects/llmeval/data/llm_cache', load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1737455567, served_model_name=meta-llama/Llama-3.3-70B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 01-21 05:33:39 ray_gpu_executor.py:134] use_ray_spmd_worker: False
INFO 01-21 05:36:32 utils.py:992] Found nccl from library libnccl.so.2
INFO 01-21 05:36:32 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(RayWorkerWrapper pid=3772972)[0m INFO 01-21 05:36:32 utils.py:992] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=3772972)[0m INFO 01-21 05:36:32 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 01-21 05:36:35 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=3772972)[0m INFO 01-21 05:36:35 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/jsalvador/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 01-21 05:36:35 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x151c34e1c2d0>, local_subscribe_port=32977, remote_subscribe_port=None)
INFO 01-21 05:36:35 model_runner.py:1014] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
[36m(RayWorkerWrapper pid=3772972)[0m INFO 01-21 05:36:35 model_runner.py:1014] Starting to load model meta-llama/Llama-3.3-70B-Instruct...
INFO 01-21 05:36:39 loader.py:1014] Loading weights with BitsAndBytes quantization.  May take a while ...
[36m(RayWorkerWrapper pid=3772972)[0m INFO 01-21 05:36:39 loader.py:1014] Loading weights with BitsAndBytes quantization.  May take a while ...
INFO 01-21 05:36:40 weight_utils.py:242] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   3% Completed | 1/30 [00:09<04:27,  9.21s/it]
Loading safetensors checkpoint shards:   7% Completed | 2/30 [00:18<04:19,  9.28s/it]
Loading safetensors checkpoint shards:  10% Completed | 3/30 [00:27<04:00,  8.89s/it]
Loading safetensors checkpoint shards:  13% Completed | 4/30 [00:37<04:05,  9.43s/it]
Loading safetensors checkpoint shards:  17% Completed | 5/30 [00:44<03:35,  8.63s/it]
Loading safetensors checkpoint shards:  20% Completed | 6/30 [00:53<03:32,  8.86s/it]
Loading safetensors checkpoint shards:  23% Completed | 7/30 [01:03<03:29,  9.11s/it]
Loading safetensors checkpoint shards:  27% Completed | 8/30 [01:11<03:16,  8.92s/it]
Loading safetensors checkpoint shards:  30% Completed | 9/30 [01:23<03:23,  9.69s/it]
Loading safetensors checkpoint shards:  33% Completed | 10/30 [01:32<03:11,  9.60s/it]
Loading safetensors checkpoint shards:  37% Completed | 11/30 [01:38<02:37,  8.30s/it]
Loading safetensors checkpoint shards:  40% Completed | 12/30 [01:48<02:38,  8.81s/it]
Loading safetensors checkpoint shards:  43% Completed | 13/30 [01:57<02:34,  9.09s/it]
Loading safetensors checkpoint shards:  47% Completed | 14/30 [02:08<02:31,  9.49s/it]
Loading safetensors checkpoint shards:  50% Completed | 15/30 [02:19<02:29,  9.98s/it]
Loading safetensors checkpoint shards:  53% Completed | 16/30 [02:29<02:20, 10.03s/it]
Loading safetensors checkpoint shards:  57% Completed | 17/30 [02:40<02:13, 10.30s/it]
Loading safetensors checkpoint shards:  60% Completed | 18/30 [02:51<02:06, 10.51s/it]
Loading safetensors checkpoint shards:  63% Completed | 19/30 [03:04<02:05, 11.45s/it]
Loading safetensors checkpoint shards:  67% Completed | 20/30 [03:14<01:47, 10.73s/it]
Loading safetensors checkpoint shards:  70% Completed | 21/30 [03:26<01:40, 11.12s/it]
Loading safetensors checkpoint shards:  73% Completed | 22/30 [03:35<01:24, 10.52s/it]
Loading safetensors checkpoint shards:  77% Completed | 23/30 [03:43<01:09,  9.91s/it]
Loading safetensors checkpoint shards:  80% Completed | 24/30 [03:53<00:58,  9.82s/it]
Loading safetensors checkpoint shards:  83% Completed | 25/30 [04:01<00:47,  9.41s/it]
Loading safetensors checkpoint shards:  87% Completed | 26/30 [04:11<00:37,  9.41s/it]
Loading safetensors checkpoint shards:  90% Completed | 27/30 [04:19<00:27,  9.19s/it]
Loading safetensors checkpoint shards:  93% Completed | 28/30 [04:29<00:18,  9.21s/it]
Loading safetensors checkpoint shards:  97% Completed | 29/30 [04:39<00:09,  9.47s/it]
Loading safetensors checkpoint shards: 100% Completed | 30/30 [04:49<00:00,  9.76s/it]
Loading safetensors checkpoint shards: 100% Completed | 30/30 [04:49<00:00,  9.65s/it]

[36m(RayWorkerWrapper pid=3772972)[0m INFO 01-21 05:36:40 weight_utils.py:242] Using model weights format ['*.safetensors']
[36m(RayWorkerWrapper pid=3772972)[0m INFO 01-21 05:41:30 model_runner.py:1025] Loading model weights took 18.4395 GB
INFO 01-21 05:41:30 model_runner.py:1025] Loading model weights took 18.4395 GB
INFO 01-21 05:41:37 distributed_gpu_executor.py:57] # GPU blocks: 20819, # CPU blocks: 1638
[[33mINFO[0m] begin collecting responses
generating responses:   0%|          | 0/378 [00:00<?, ?it/s]generating responses:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 141/378 [23:03<38:45,  9.81s/it]generating responses:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 142/378 [46:09<1:32:53, 23.62s/it]generating responses:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 143/378 [1:10:36<2:52:21, 44.01s/it]generating responses:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 144/378 [1:34:47<4:40:11, 71.84s/it]generating responses:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 145/378 [1:55:14<6:42:14, 103.58s/it]generating responses:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 146/378 [2:06:00<7:59:47, 124.08s/it]generating responses:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 147/378 [2:22:22<10:46:52, 168.02s/it]generating responses:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 148/378 [2:37:41<14:00:10, 219.18s/it]generating responses:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 149/378 [2:51:57<17:32:16, 275.71s/it]generating responses:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 150/378 [3:06:39<21:46:42, 343.87s/it]generating responses:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 151/378 [3:20:18<25:49:55, 409.67s/it]generating responses:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 152/378 [3:32:12<28:52:15, 459.89s/it]generating responses:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 153/378 [3:46:53<33:46:16, 540.34s/it]generating responses:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 154/378 [3:58:25<35:38:26, 572.80s/it]generating responses:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 155/378 [4:13:51<40:36:45, 655.63s/it]generating responses:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156/378 [4:25:41<41:15:53, 669.16s/it]generating responses:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157/378 [4:40:40<44:48:01, 729.78s/it]generating responses:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 158/378 [4:55:02<46:48:37, 765.99s/it]generating responses:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 159/378 [5:06:45<45:31:06, 748.25s/it]generating responses:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/378 [5:24:30<50:48:57, 839.16s/it]generating responses:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 161/378 [5:40:55<53:08:28, 881.61s/it]generating responses:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 162/378 [5:59:43<57:13:43, 953.81s/it]generating responses:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 163/378 [6:19:27<61:01:12, 1021.73s/it]generating responses:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 164/378 [6:40:53<65:23:07, 1099.94s/it]generating responses:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 165/378 [7:01:56<67:57:50, 1148.69s/it]generating responses:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 166/378 [7:24:30<71:14:50, 1209.86s/it]generating responses:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 167/378 [7:46:55<73:17:04, 1250.35s/it]generating responses:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 168/378 [8:09:16<74:30:32, 1277.30s/it]generating responses:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 169/378 [8:30:35<74:10:53, 1277.77s/it]generating responses:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 170/378 [8:52:38<74:36:33, 1291.32s/it]generating responses:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 171/378 [9:14:36<74:43:00, 1299.42s/it]generating responses:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 172/378 [9:34:22<72:25:03, 1265.55s/it]generating responses:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 173/378 [9:55:58<72:34:20, 1274.44s/it]generating responses:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 174/378 [10:17:03<72:03:47, 1271.70s/it]generating responses:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 175/378 [10:36:33<69:59:13, 1241.15s/it]generating responses:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 176/378 [10:54:58<67:21:15, 1200.37s/it]generating responses:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 177/378 [11:17:21<69:24:43, 1243.20s/it]generating responses:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 178/378 [11:40:15<71:14:52, 1282.46s/it]generating responses:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 179/378 [12:02:07<71:22:22, 1291.17s/it]generating responses:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 180/378 [12:25:03<72:25:20, 1316.77s/it]generating responses:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 181/378 [12:35:31<60:44:38, 1110.04s/it]generating responses:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 182/378 [12:45:48<52:23:27, 962.28s/it] generating responses:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 183/378 [12:56:04<46:29:22, 858.27s/it]generating responses:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 184/378 [13:06:28<42:28:01, 788.05s/it]generating responses:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 185/378 [13:16:49<39:33:29, 737.87s/it]generating responses:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 186/378 [13:27:00<37:19:25, 699.82s/it]generating responses:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 187/378 [13:37:24<35:55:35, 677.15s/it]generating responses:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 188/378 [13:47:38<34:44:27, 658.25s/it]generating responses:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 189/378 [13:58:00<33:58:27, 647.13s/it]generating responses:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 190/378 [14:08:20<33:22:49, 639.20s/it]generating responses:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 191/378 [14:23:25<37:20:22, 718.83s/it]generating responses:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 192/378 [14:33:37<35:29:28, 686.93s/it]generating responses:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 193/378 [14:43:54<34:12:40, 665.73s/it]generating responses:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 194/378 [14:54:05<33:11:46, 649.49s/it]generating responses:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 195/378 [15:09:09<36:53:34, 725.76s/it]generating responses:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 196/378 [15:19:39<35:14:49, 697.19s/it]generating responses:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 197/378 [15:30:04<33:57:39, 675.47s/it]generating responses:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 198/378 [15:40:23<32:55:41, 658.56s/it]generating responses:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 199/378 [15:51:23<32:45:19, 658.77s/it]generating responses:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 200/378 [16:02:49<32:58:42, 666.98s/it]generating responses:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 201/378 [16:16:26<35:00:28, 712.02s/it]generating responses:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 202/378 [16:26:52<33:32:38, 686.13s/it]generating responses:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 203/378 [16:37:09<32:21:07, 665.53s/it]generating responses:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 204/378 [16:47:33<31:34:01, 653.11s/it]generating responses:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 205/378 [16:58:19<31:16:56, 650.96s/it]generating responses:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 206/378 [17:09:04<31:00:55, 649.16s/it]generating responses:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 207/378 [17:22:43<33:15:05, 700.03s/it]generating responses:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 208/378 [17:33:22<32:11:34, 681.73s/it]generating responses:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 209/378 [17:45:39<32:47:17, 698.45s/it]generating responses:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 210/378 [17:56:19<31:46:11, 680.78s/it]generating responses:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 211/378 [18:09:03<32:44:05, 705.66s/it]generating responses:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 212/378 [18:23:02<34:22:56, 745.64s/it]generating responses:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 213/378 [18:36:04<34:41:08, 756.78s/it]generating responses:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 214/378 [18:49:01<34:44:37, 762.67s/it]generating responses:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 215/378 [19:01:59<34:44:50, 767.43s/it]generating responses:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 216/378 [19:14:58<34:41:15, 770.84s/it]generating responses:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 217/378 [19:37:04<41:55:39, 937.52s/it]generating responses:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 218/378 [19:59:25<47:02:21, 1058.39s/it]generating responses:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 219/378 [20:22:35<51:08:49, 1158.05s/it]generating responses:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 220/378 [20:44:56<53:13:40, 1212.79s/it]generating responses:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 221/378 [21:08:06<55:12:36, 1265.97s/it]generating responses:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 222/378 [21:31:13<56:26:07, 1302.35s/it]generating responses:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 223/378 [21:54:03<56:56:56, 1322.69s/it]generating responses:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 224/378 [22:17:03<57:18:36, 1339.72s/it]generating responses:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 225/378 [22:40:12<57:34:06, 1354.55s/it]generating responses:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 226/378 [23:02:15<56:47:14, 1344.96s/it]generating responses:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 227/378 [23:25:13<56:49:45, 1354.87s/it]generating responses:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 228/378 [23:48:29<56:58:09, 1367.26s/it]generating responses:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 229/378 [24:11:10<56:30:30, 1365.30s/it]generating responses:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 230/378 [24:33:54<56:07:08, 1365.06s/it]generating responses:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 231/378 [24:56:12<55:24:17, 1356.86s/it]generating responses:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 232/378 [25:18:55<55:06:35, 1358.88s/it]generating responses:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 233/378 [25:43:55<56:25:43, 1400.99s/it]generating responses:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 234/378 [26:08:09<56:40:49, 1417.01s/it]generating responses:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 235/378 [26:23:09<50:07:14, 1261.78s/it]generating responses:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 236/378 [26:33:31<42:12:37, 1070.13s/it]generating responses:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 237/378 [26:44:26<37:01:42, 945.41s/it] generating responses:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 238/378 [26:55:04<33:10:54, 853.25s/it]generating responses:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 239/378 [27:11:39<34:35:27, 895.88s/it]generating responses:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 240/378 [27:26:22<34:11:17, 891.87s/it]generating responses:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 241/378 [27:37:08<31:07:55, 818.07s/it]generating responses:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 242/378 [27:48:57<29:40:15, 785.41s/it]generating responses:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 243/378 [28:09:03<34:11:15, 911.67s/it]generating responses:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 244/378 [28:21:20<31:58:56, 859.22s/it]generating responses:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 245/378 [28:34:03<30:40:49, 830.45s/it]generating responses:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 246/378 [28:49:25<31:27:15, 857.85s/it]generating responses:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 247/378 [29:01:40<29:52:31, 821.00s/it]generating responses:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 248/378 [29:17:33<31:04:20, 860.47s/it]generating responses:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 249/378 [29:28:40<28:45:11, 802.42s/it]generating responses:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 250/378 [29:49:47<33:29:10, 941.80s/it]generating responses:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 251/378 [30:06:28<33:50:58, 959.52s/it]generating responses:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 252/378 [30:25:52<35:43:59, 1020.95s/it]generating responses:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 253/378 [30:49:13<39:24:22, 1134.90s/it]generating responses:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 254/378 [31:11:35<41:13:43, 1196.96s/it]generating responses:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 255/378 [31:34:11<42:31:37, 1244.70s/it]generating responses:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 256/378 [31:53:53<41:32:58, 1226.06s/it]generating responses:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 257/378 [32:17:23<43:03:36, 1281.13s/it]generating responses:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 258/378 [32:41:09<44:09:08, 1324.57s/it]generating responses:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 259/378 [33:01:08<42:32:38, 1287.05s/it]generating responses:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 260/378 [33:23:23<42:39:34, 1301.48s/it]generating responses:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 261/378 [33:46:06<42:53:42, 1319.85s/it]generating responses:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 262/378 [34:08:55<43:00:08, 1334.56s/it]slurmstepd: error: *** JOB 412642 ON evc35 CANCELLED AT 2025-01-22T16:12:38 DUE TO TIME LIMIT ***
